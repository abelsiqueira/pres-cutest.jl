\myframe{
  \begin{multicols}{2}
  \tableofcontents
  \end{multicols}
}

\AtBeginSection[]{
  \begin{frame}
    \begin{multicols}{2}
    \tableofcontents[currentsection]
    \end{multicols}
  \end{frame}
}


\section{Workflow}

\myframe{
   ``... We should forget about the small efficiencies, say about 97\% of the
    time: {\bf premature optimization is the root of all evil}...'' \\
    Donald Knuth \\

  ``The strategy is definitely: first {\bf make it work}, then {\bf make it
    right}, and, finally, {\bf make it fast}.'' \\
    Kent Beck
}

\subsection{Test Driven Development}

\myframectr{
\begin{center}
\begin{tikzpicture}
  \node (test) at (0, 2) {Create a test};
  \node (fix) at (1.6, -1) {Fix the test};
  \node (refactor) at (-1.6, -1) {Refactor};
  \draw[->] (test) to[out=-20,in=60] (fix);
  \draw[->] (fix) to[out=240,in=300] (refactor);
  \draw[->] (refactor) to[out=120,in=200] (test);
\end{tikzpicture}
\end{center}
}

\subsection{Problems as blocks}

\myframe{
  \ctr{Problems as blocks}
  \begin{itemize}
    \item Simplest representative problems;
    \item Classes of problems;
    \item Selection of problems from some specific repository.
  \end{itemize}
}

\myframe{
  \ctr{Test nonzero exit flag}
  \begin{itemize}
    \item Infinite loop;
    \item Budget limitations;
    \item Domain error.
  \end{itemize}
}

\myframe{
  \ctr{Scale the problem}
  \begin{itemize}
    \item Dense matrices;
    \item Inplace operations;
    \item Small efficiencies.
  \end{itemize}
}

\subsection{Outline}

\begin{frame}[t,fragile]
  \ctr{Outline}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \item<1-> Use a test library (\mintinline{julia}{FactCheck.jl});
    \item<2->
    \begin{enumerate}
      \item Write small problem;
      \item Write simplest code solving it (theory \yes, efficiency \no);
      \item Repeat
    \end{enumerate}
    \item<3-> Write limitation tests (time, iteration, etc.) and code for it;
    \item<4->
    \begin{enumerate}
      \item Write class os problems (hundreds of runs, randomized, larger size);
      \item Write code solving it;
      \item Repeat
    \end{enumerate}
    \item<5->
    \begin{enumerate}
      \item Choose problems from specific repository;
      \item Try to solve it;
      \item Repeat
    \end{enumerate}
    \item<6-> Improve the code.
  \end{enumerate}
\end{frame}

\section{Framework}

\subsection{Julia}

\myframectr{
  \begin{itemize}
    \item High level, High performance;
    \item Open source, multiplatform;
    \item Great C/Fortran interface;
    \item Easy syntax;
    \item Good practices (git, automated testing, code coverage);
  \end{itemize}
}

\begin{frame}[fragile]
  \ctr{Framework}
  \begin{itemize}
    \item Develop with {\color{red} \mintinline{julia}{Optimize.jl}}
    \item Create the tests with {\color{red} \mintinline{julia}{NLPModels.jl}};
    \item Easily access CUTEst with {\color{red} \mintinline{julia}{CUTEst.jl}}.
  \end{itemize}
\end{frame}

\subsection{Optimize.jl}

\myframe{
  \ctr{Optimize.jl}
  \begin{itemize}
    \item Methods;
    \item Auxiliary algorithms and tools (trust region, line search, etc.);
    \item Testing and benchmarking.
  \end{itemize}
}

\subsection{NLPModels.jl}

\begin{frame}[fragile]
  \ctr{NLPModels.jl}
  \begin{itemize}
    \item Define \mintinline{julia}{AbstractNLPModel};
    \item Define \mintinline{julia}{JuMPNLPModel} and
      \mintinline{julia}{SimpleNLPModel};
    \item AMPL and CUTEst models are derived from it;
    \item Allows future models.
  \end{itemize}
\end{frame}

\myframe{
  \ctr{NLPModels.jl}
\begin{center}
\begin{tikzpicture}
  \node[draw,rectangle] (abstract) at (-4,0) {AbstractNLPModel};
  \node[draw,rectangle] (jumpnlp) at (0,1) {JuMPNLPModel};
  \node[draw,rectangle] (simple) at (0,-1) {SimpleNLPModel};
  \node[red,draw,rectangle] (ampl) at (0,-2) {AmplModel};
  \node[red,draw,rectangle] (cutest) at (0,0) {CUTEstModel};
  \node[draw,rectangle] (opt) at (-4,-2) {Optimize};
  \node[dashed,draw,rectangle] (jump) at (4,1) {JuMP};
  \node[dashed,draw,rectangle] (mpb) at (4,0) {MathProgBase};
  \node[dashed,draw,rectangle] (ipopt) at (4,-1) {IPOPT};
  \draw[thick,->] (jumpnlp) -- (abstract);
  \draw[thick,->] (simple) -- (abstract);
  \draw[thick,->] (ampl) -- (abstract);
  \draw[thick,->] (cutest) -- (abstract);
  \draw[thick,->] (abstract) -- (opt);
  \draw[thick,->] (cutest) -- (mpb);
  \draw[thick,->] (jump) -- (jumpnlp);
  \draw[thick,->] (jump) -- (mpb);
  \draw[thick,->] (mpb) -- (ipopt);
\end{tikzpicture}
\end{center}
}

\section{CUTEst.jl}

\subsection{CUTEst}

\myframectr{
  \begin{itemize}
    \item Repository of Nonlinear Optimization problems;
    \item Provides subroutines to obtain the problem's information;
    \item Decodes the problem, compiles your code with the problem's and runs
      your main code;
    \item Widely used.
  \end{itemize}
}

\subsection{CUTEst.jl}

\myframectr{
  \begin{itemize}
    \item Easy to install;
    \item Easy to use;
    \item Helps in many stages of the development.
  \end{itemize}
}

\begin{frame}[t,fragile]
  \ctr{CUTEst (Fortran)}
  \begin{minted}{fortran}
... !open problem and define variables
CALL cutest_cdimen(st, ifile, n, m)
if (m.GT.0) THEN
  STOP
ENDIF
CALL cutest_usetup(st, ifile, 7, 11, n, x, bl, bu)
CALL cutest_ufn(st, n, x, f)
... !close and end
\end{minted}
\end{frame}

\begin{frame}[t,fragile]
  \ctr{CUTEst.jl}
  \begin{minted}{julia}
Pkg.add("CUTEst") # Once (Eventually)
\end{minted}
\begin{minted}{julia}
using CUTEst

nlp = CUTEstModel("ROSENBR")
x = nlp.meta.x0

f = obj(nlp, x)
cutest_finalize(nlp)
\end{minted}
\end{frame}

\subsection{Flavors}

\begin{frame}[t,fragile]
  \ctr{Flavors}
\begin{minted}{fortran}
CALL cutest_ufn(st, n, x, f)
CALL cutest_cfn(st, n, m, x, f, c)
\end{minted}
\ctr{Wrapper}
\begin{minted}{julia}
st = Cint[0]
f = [0.0]
c = zeros(m)
ufn(st, Cint[n], x, f)
cfn(st, Cint[n], Cint[m], x, f, c)
\end{minted}
\end{frame}

\begin{frame}[t,fragile]
  \ctr{Flavors}
\begin{minted}{fortran}
CALL cutest_ufn(st, n, x, f)
CALL cutest_cfn(st, n, m, x, f, c)
\end{minted}
\ctr{Julian way}
\begin{minted}{julia}
f = ufn(nlp, x)
f = ufn(n, x)
f, c = cfn(nlp, x)
f, c = cfn(n, m, x)
f = cfn!(nlp, x, c) # Inplace
f = cfn!(n, m, x, c) # Inplace
\end{minted}
\end{frame}

\begin{frame}[t,fragile]
  \ctr{Flavors}
\begin{minted}{fortran}
CALL cutest_ufn(st, n, x, f)
CALL cutest_cfn(st, n, m, x, f, c)
\end{minted}
\ctr{AbstractModel way}
\begin{minted}{julia}
f = obj(nlp, x)
c = cons(nlp, x)
f = objcons(nlp, x) # If unconstrained
f, c = objcons(nlp, x) # If constrained
\end{minted}
\end{frame}

\subsection{Some functions}

\begin{frame}[t,fragile]
  \ctr{Some functions}
  \begin{minted}{julia}
g = grad(nlp, x)
H = hess(nlp, x) # Sparse
H = hess(nlp, x, y) # Sparse
hrow, hcol, hval = hess_coord(nlp, x)
hrow, hcol, hval = hess_coord(nlp, x, y)
J = jac(nlp, x) # Sparse
jrow, jcol, jval = jac_coord(nlp, x)
Hv = hprod(nlp, x, v)
\end{minted}
\end{frame}

\section{Practical example}

\subsection{TRON}

\myframe{
  \ctr{TRON: A practical example}
    {\bf Newton's Method for Large Bound-Constrained Optimization
      Problems} \\
      Chih-Jen Lin and Jorge J. MorÃ© \\
      {\it SIAM Journal on Optimization} Vol. 9, No. 4, pp. 1100-1127, 1999.
}

\myframe{
  \[ \min f(x) \quad \mbox{s. to} \quad x \in \Omega, \]
  where $\Omega$ is
  \[ \Omega = \{ x \in \mathbb{R}^n \mid \ell \leq x \leq u \}, \]
  but can be extended to
  \[ \Omega = \{ x \in \mathbb{R}^n \mid \ell \leq c_i^Tx \leq u,
  \ i \in \mathcal{I} \}. \]
}

\myframe{
  \ctr{Outline of iteration $k$}
  \begin{enumerate}
    \item Compute a model
      \[ m_k(d) = \frac{1}{2}d^TB_kd + d^Tg_k. \]
    \item Compute a gradient step $s_k = P[x_k - \alpha_kg_k] - x_k$ such that
      \[ m_k(s_k) \leq \mu_0 g_k^Ts_k, \qquad \mbox{and} \qquad
        \Vert s_k \Vert \leq \mu_1\Delta_k. \]
    \item Compute a step $d_k$ better than $s_k$, i.e., further minimizing
      $m_k$ with $\Vert d_k \Vert \leq \mu_1\Delta_k$,
      and without leaving the bounds at $x_k + s_k$.
    \item Update $x_k$ and $\Delta_k$ using Trust Region rules.
  \end{enumerate}
}

\subsection{Make it work}

\myframe{
  \ctr{Simplest problems}
  \begin{itemize}
    \item $\min \ f(x) = \frac{1}{2}(x_1^2 + x_2^2)$;
    \item $\min \ f(x) = \frac{1}{2}(x_1^2 + x_2^2),$ subject to $1 \leq x_1,x_2 \leq 2$;
    \item $\min \ f(x) = \frac{1}{2}(x_1^2 + x_2^2),$ subject to $-1 \leq x_1,x_2 \leq 2$;
    \item $\min \ f(x) = \frac{1}{2}(x_1^2 + x_2^2),$ subject to $-1 \leq x_1
      \leq 1, \ 1 \leq x_2 \leq 2$;
    \item $\min \ f(x) = \frac{1}{2}(x_1^2 + x_2^2),$ subject to $0 \leq x_1
      \leq 1, \ 1 \leq x_2 \leq 2$;
    \item Rosenbrock with and without bounds;
  \end{itemize}
}

\myframe{
  \ctr{Minimum implementation}
  \begin{enumerate}
    \item Use $B_k = \nabla^2f(x_k)$;
    \item Use simple backtracking to find $s_k$;
    \item Use $d_k = s_k$;
    \item Use the Trust Region implemented in the framework (similar enough).
  \end{enumerate}
}

\begin{frame}[t,fragile,allowframebreaks]
  \ctr{FactCheck}
  \begin{minted}{julia}
using FactCheck

facts("Simple test") do
  x0 = [1.2; 1.8]
  f(x) = dot(x,x)/2
  g(x) = x
  H(x) = eye(2)
  l = [1.0; 1.0]
  u = [2.0; 2.0]
  nlp = SimpleNLPModel(x0, f, grad=g, hess=H, lvar=l,
    uvar=u)

  x, fx, dual = tron(nlp)
  @fact x --> roughly(l)
  @fact fx --> roughly(f(l))
  @fact dual --> roughly(0.0)
end
\end{minted}
\end{frame}

\begin{frame}[t,fragile,allowframebreaks]
  \ctr{Minimum implementation}
\begin{minted}{julia}
function tron(nlp; m0 = 1e-2, m1 = 1.0)
  f(x) = obj(nlp, x)
  g(x) = grad(nlp, x)
  H(x) = hess(nlp, x)

  P(x) = max(min(x, u), l)
  dual(x) = norm(P(x - g(x)) - x)

  tr = TrustRegion(100.0)
  D() = get_property(tr, :radius)

  x = nlp.meta.x0
  while dual(x) > 1e-6
    q(d) = 0.5*dot(d, H(x) * d) + dot(d, g(x))
    s(a) = P(x - a*g(x)) - x

    a = 1
    while q(s(a)) > m0*dot(g(x), s(a)) ||
        norm(s(a)) > m1*D()
      a *= 0.9
    end
    xp = x + s(a)
    rho = ratio(f(x), f(xp), q(xp-x))
    if acceptable(tr, rho)
      x = xp
    end
    update!(tr, rho, norm(s(a)))
  end
  return x, f(x), dual(x)
end
\end{minted}
\end{frame}

\subsection{Make it right}

\begin{frame}[fragile]
  \ctr{Limits}
  \begin{minted}{julia}
f(x) = begin sleep(0.1); sum(exp(x)) end
g(x) = begin sleep(0.1); exp(x) end
H(x) = begin sleep(0.1); spdiagm(exp(x), 0, 10, 10) end
x0 = 10*ones(10)
\end{minted}
\end{frame}

\myframe{
  \ctr{Class os tests}
  \begin{itemize}
    \item $\min \ f(x) = \frac{1}{2}(x-r)^TQ^T\Lambda Q(x-r) + 1$,
    where
    \begin{itemize}
      \item $r = (1,\dots,1)^T$;
      \item $Q$ is an orthogonal matrix;
      \item $\Lambda = \mbox{diag}(10^{-2}, \dots, 1)^T$
    \end{itemize}
  \item $\min \ f(x) = \frac{1}{2}x^TBx + g^Tx$, subject to
    $\ell \leq x \leq u$,
  where
  \begin{itemize}
    \item $B = Q^T\Lambda Q > 0$.
    \item Build solution and choose $g$
  \end{itemize}
  \item Generalized rosenbrock with bounds.
  \end{itemize}
}

\begin{frame}[fragile]
  \ctr{Improvements}
  \begin{itemize}
    \item Matrix-free (hprod instead of hess);
    \item Store repeated function calls;
    \item Use inplace operations;
  \end{itemize}
\end{frame}

\section{Future work}

\subsection{Future work}

\myframe{
  \ctr{Future Work}

  \begin{itemize}
    \item Implement various methods;
    \item Create a documentation;
    \item Benchmark many methods;
    \item Problem selection.
  \end{itemize}
}

\subsection{Thanks}
