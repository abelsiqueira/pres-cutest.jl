\AtBeginSection[]
{
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\section{Introduction}

\subsection{Julia}

\myframectr{
  \begin{itemize}
    \item High level, High performance
    \item Great C/Fortran interface
    \item Easy syntax
  \end{itemize}
}

\subsection{CUTEst}

\myframectr{
  \begin{itemize}
    \item Repository of Nonlinear Optimization problems;
    \item Provides subroutines to obtain the problem's information;
    \item Decodes the problem, compiles your code with the problem's and runs
      your main code;
    \item Widely used.
  \end{itemize}
}

\subsection{CUTEst.jl}

\myframectr{
  \begin{itemize}
    \item Easy to install;
    \item Easy to use;
    \item Easy to optimize;
  \end{itemize}
}

\section{Why CUTEst.jl}

\subsection{CUTEst}

\begin{frame}[t,fragile]
  \ctr{CUTEst (Fortran)}
\begin{lstlisting}[language=Fortran]
CALL cutest_cdimen(st, ifile, n, m)
if (m.GT.0) THEN
  STOP
ENDIF
CALL cutest_usetup(st, ifile, 7, 11, n, x, bl, bu)
CALL cutest_ufn(st, n, x, f)
\end{lstlisting}
\end{frame}

\begin{frame}[t,fragile]
  \ctr{CUTEst.jl}
\begin{lstlisting}
Pkg.add("CUTEst") # Once
\end{lstlisting}
\begin{lstlisting}
using CUTEst

nlp = CUTEstModel("ROSENBR")
x = nlp.meta.x0

f = obj(nlp, x)
\end{lstlisting}
\end{frame}

\begin{frame}[t,fragile]
  \ctr{Example}
\lstinputlisting{src/newton.jl}
\begin{lstlisting}
f(x) = obj(nlp, x)
g(x) = grad(nlp, x)
H(x) = hess(nlp, x)
x = newton(f, g, H, nlp.meta.x0)
\end{lstlisting}
\end{frame}

\begin{frame}[t,fragile]
  \ctr{Example}
\lstinputlisting{src/newton.jl}
\begin{lstlisting}
f(x) = ufn(nlp, x) # Familiar names
g(x) = ugr(nlp, x) # are here too
H(x) = udh(nlp, x, nlp.meta.nvar)
x = newton(f, g, H, nlp.meta.x0)
\end{lstlisting}
\end{frame}

\subsection{Memory efficient}

\begin{frame}[t,fragile]
  \ctr{Example with inplace}
\lstinputlisting{src/newtoninplace.jl}
\end{frame}

\begin{frame}[t,fragile]
  \ctr{Example with inplace}
\begin{lstlisting}
f(x) = ufn(nlp, x)
g!(x, gx) = ugr!(nlp, x, gx)
H!(x, Hx) = udh!(nlp, x, nlp.meta.nvar, Hx)
x = newton(f, g!, H!, nlp.meta.x0)
\end{lstlisting}
\end{frame}

\subsection{The Fortran wrapper}

\begin{frame}[t,fragile]
  \ctr{Fortran wrapper}
\begin{lstlisting}
io_err = Cint[0]
n = Cint[nlp.meta.nvar]
f = [0.0]
ufn(io_err, n, x, f)
\end{lstlisting}
\end{frame}

\section{Workflow}

\myframe{
   ``... We should forget about the small efficiencies, say about 97\% of the
    time: {\bf premature optimization is the root of all evil}...'' \\
    Donald Knuth
}

\subsection{Test Driven Development}

\myframectr{
  \begin{itemize}
    \item {\it Create a failing test; Fix the test; Refactor code; Repeat.}
    \item Optimization doesn't have small building blocks;
    \item Simplest representative problem you should be able to solve;
    \item Classes of problems you should be able to solve;
    \item Selection of CUTEst problems you should be able to solve.
  \end{itemize}
}

\myframe{
  \ctr{Nonzero exit flag}
  \begin{itemize}
    \item Infinite loop;
    \item Wrong derivative;
    \item Budget limitations;
    \item Domain error.
  \end{itemize}
}

\myframe{
  \ctr{Scaling}
  \begin{itemize}
    \item Dense matrices;
    \item Inplace operations;
    \item Small efficiencies.
  \end{itemize}
}

\section{TRON}

\myframe{
  \ctr{A practical example}
    {\bf Newton's Method for Large Bound-Constrained Optimization
      Problems} \\
      Chih-Jen Lin and Jorge J. MorÃ© \\
      {\it SIAM Journal on Optimization} Vol. 9, No. 4, pp. 1100-1127, 1999.
}

\myframe{
  \[ \min f(x) \quad \mbox{s. to} \quad x \in \Omega, \]
  where $\Omega$ is
  \[ \Omega = \{ x \in \mathbb{R}^n \mid \ell \leq x \leq u \}, \]
  but can be extended to
  \[ \Omega = \{ x \in \mathbb{R}^n \mid \ell \leq c_i^Tx \leq u,
  \ i \in \mathcal{I} \}. \]
}

\myframe{
  \ctr{Outline of iteration $k$}
  \begin{enumerate}
    \item Compute a model
      \[ m_k(d) = \frac{1}{2}d^TB_kd + d^Tg_k. \]
    \item Compute a gradient step $s_k = P[x_k - \alpha_kg_k] - x_k$ such that
      \[ m_k(s_k) \leq \mu_0 g_k^Ts_k, \qquad \mbox{and} \qquad
        \Vert s_k \Vert \leq \mu_1\Delta_k. \]
    \item Compute a step $d_k$ better than $s_k$, i.e., further minimizing
      $m_k$ with $\Vert d_k \Vert \leq \mu_1\Delta_k$.
    \item Update $x_k$ and $\Delta_k$ using Trust Region rules.
  \end{enumerate}
}

\myframe{
  \ctr{Simplest problems}
  \begin{itemize}
    \item $f(x) = \frac{1}{2}(x_1^2 + x_2^2)$.
  \end{itemize}
}

\begin{frame}[t,fragile]
  \ctr{FactCheck}
\begin{lstlisting}[basicstyle=\scriptsize\tt]
facts("Simple test") do
  x0 = [1.0; 2.0]
  f(x) = dot(x,x)/2
  g(x) = x
  H(x) = eye(2)
  nlp = SimpleNLPModel(x0, f, grad=g, hess=H)

  x, fx, dual = tron(nlp)
  @fact x --> roughly(zeros(2))
  @fact fx --> roughly(0.0)
  @fact dual --> roughly(0.0)
end
\end{lstlisting}
\end{frame}

\begin{frame}[t,fragile]
  \ctr{FactCheck}
\begin{lstlisting}[basicstyle=\scriptsize\tt]
  ...
  @fact dual --> roughly(0.0)

  # Another simple test
  l = [0.5; 0.25]
  nlp = SimpleNLPModel(x0, f, grad=g, hess=H, lvar = l)

  x, fx, dual = tron(nlp)
  @fact x --> roughly(l)
  @fact fx --> roughly(f(l))
  @fact dual --> roughly(0.0)
end
\end{lstlisting}
\end{frame}

\subsection{Thanks}
